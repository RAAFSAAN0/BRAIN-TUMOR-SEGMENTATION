{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":4421337,"sourceType":"datasetVersion","datasetId":2589774},{"sourceId":4462988,"sourceType":"datasetVersion","datasetId":2612387},{"sourceId":4463116,"sourceType":"datasetVersion","datasetId":2612457},{"sourceId":8236347,"sourceType":"datasetVersion","datasetId":4885230},{"sourceId":8250732,"sourceType":"datasetVersion","datasetId":4895513},{"sourceId":8251960,"sourceType":"datasetVersion","datasetId":4896439},{"sourceId":8251969,"sourceType":"datasetVersion","datasetId":4896447}],"dockerImageVersionId":30214,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport PIL\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage \nimport skimage.transform as skTrans22\nfrom skimage.transform import rotate\nfrom skimage.transform import resize\nfrom PIL import Image, ImageOps  \n\n# neural imaging\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\n\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\n# Make numpy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:01.309376Z","iopub.execute_input":"2025-02-03T16:20:01.309726Z","iopub.status.idle":"2025-02-03T16:20:01.318993Z","shell.execute_reply.started":"2025-02-03T16:20:01.3097Z","shell.execute_reply":"2025-02-03T16:20:01.317888Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'NOT tumor',   \n    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE - RED\n    2 : 'EDEMA',  # Green\n    3 : 'ENHANCING' # original 4 -> converted into 3 later, Yellow\n}\n\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n#VOLUME_SLICES = 100  \nVOLUME_SLICES = 48\nVOLUME_START_AT = 22 # first slice of volume that we will include\n\nIMG_HEIGHT = 128\nIMG_WIDTH = 128\nIMG_DEPTH = 48\nIMG_CHANNELS = 3","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:01.320658Z","iopub.execute_input":"2025-02-03T16:20:01.321259Z","iopub.status.idle":"2025-02-03T16:20:01.336166Z","shell.execute_reply.started":"2025-02-03T16:20:01.321233Z","shell.execute_reply":"2025-02-03T16:20:01.335472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\nVALIDATION_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/'\n# TRAIN_DATASET_PATH = '../input/brats2020-simples/'\n# VALIDATION_DATASET_PATH = '../input/brats2020-simples/'","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:01.337506Z","iopub.execute_input":"2025-02-03T16:20:01.337784Z","iopub.status.idle":"2025-02-03T16:20:01.348574Z","shell.execute_reply.started":"2025-02-03T16:20:01.337762Z","shell.execute_reply":"2025-02-03T16:20:01.347624Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii').get_fdata()\ntest_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\ntest_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\ntest_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\ntest_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:01.349732Z","iopub.execute_input":"2025-02-03T16:20:01.350063Z","iopub.status.idle":"2025-02-03T16:20:01.653238Z","shell.execute_reply.started":"2025-02-03T16:20:01.35003Z","shell.execute_reply":"2025-02-03T16:20:01.652431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\nslice_w = 25\nax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')\nax1.set_title('Image flair')\nax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')\nax2.set_title('Image t1')\nax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\nax3.set_title('Image t1ce')\nax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\nax4.set_title('Image t2')\nax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\nax5.set_title('Mask')","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:01.656467Z","iopub.execute_input":"2025-02-03T16:20:01.657237Z","iopub.status.idle":"2025-02-03T16:20:02.210194Z","shell.execute_reply.started":"2025-02-03T16:20:01.657188Z","shell.execute_reply":"2025-02-03T16:20:02.209011Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:02.211619Z","iopub.execute_input":"2025-02-03T16:20:02.212016Z","iopub.status.idle":"2025-02-03T16:20:03.271226Z","shell.execute_reply.started":"2025-02-03T16:20:02.211974Z","shell.execute_reply":"2025-02-03T16:20:03.270421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:03.272308Z","iopub.execute_input":"2025-02-03T16:20:03.272665Z","iopub.status.idle":"2025-02-03T16:20:04.183275Z","shell.execute_reply.started":"2025-02-03T16:20:03.272626Z","shell.execute_reply":"2025-02-03T16:20:04.182418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii')\nnimask = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii')\n\nfig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n\n\nnlplt.plot_anat(niimg,\n                title='BraTS20_Training_001_flair.nii plot_anat',\n                axes=axes[0])\n\nnlplt.plot_epi(niimg,\n               title='BraTS20_Training_001_flair.nii plot_epi',\n               axes=axes[1])\n\nnlplt.plot_img(niimg,\n               title='BraTS20_Training_001_flair.nii plot_img',\n               axes=axes[2])\n\nnlplt.plot_roi(nimask, \n               title='BraTS20_Training_001_flair.nii with mask plot_roi',\n               bg_img=niimg, \n               axes=axes[3], cmap='Paired')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:04.184666Z","iopub.execute_input":"2025-02-03T16:20:04.185016Z","iopub.status.idle":"2025-02-03T16:20:14.609943Z","shell.execute_reply.started":"2025-02-03T16:20:04.184983Z","shell.execute_reply":"2025-02-03T16:20:14.609079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Dice Coefficient Metric\ndef dice_coef(y_true, y_pred, smooth=1e-9):\n    y_pred = tf.cast(y_pred >= 0.5, dtype=tf.float32)  # Threshold predictions\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(dice)\n\n# Jaccard Index (IoU) Metric\ndef jaccard_coef(y_true, y_pred, smooth=1e-9):\n    y_pred = tf.cast(y_pred >= 0.5, dtype=tf.float32)  # Threshold predictions\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3]) - intersection\n    iou = (intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(iou)\n\n# Precision Metric\ndef precision(y_true, y_pred):\n    y_pred = tf.cast(y_pred >= 0.5, dtype=tf.float32)  # Threshold predictions\n    true_positives = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    predicted_positives = tf.reduce_sum(y_pred, axis=[1, 2, 3])\n    precision = true_positives / (predicted_positives + 1e-9)\n    return tf.reduce_mean(precision)\n\n# Sensitivity (Recall) Metric\ndef sensitivity(y_true, y_pred):\n    y_pred = tf.cast(y_pred >= 0.5, dtype=tf.float32)  # Threshold predictions\n    true_positives = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    actual_positives = tf.reduce_sum(y_true, axis=[1, 2, 3])\n    sensitivity = true_positives / (actual_positives + 1e-9)\n    return tf.reduce_mean(sensitivity)\n\n# Specificity Metric\ndef specificity(y_true, y_pred):\n    y_pred = tf.cast(y_pred >= 0.5, dtype=tf.float32)  # Threshold predictions\n    true_negatives = tf.reduce_sum((1 - y_true) * (1 - y_pred), axis=[1, 2, 3])\n    false_positives = tf.reduce_sum((1 - y_true) * y_pred, axis=[1, 2, 3])\n    specificity = true_negatives / (true_negatives + false_positives + 1e-9)\n    return tf.reduce_mean(specificity)\n\n# Define Per-Class Dice Coefficients with Unique Names\ndef dice_coef_per_class(class_index, name):\n    def dice_coef_class(y_true, y_pred):\n        y_true_class = y_true[:, :, :, class_index]\n        y_pred_class = y_pred[:, :, :, class_index]\n        return dice_coef(y_true_class, y_pred_class)\n    \n    # Assign a unique name to the metric\n    dice_coef_class.__name__ = name\n    return dice_coef_class\n\n# BCE + Dice Loss\ndef bce_dice_loss(y_true, y_pred):\n    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    dice_loss = 1.0 - dice_coef(y_true, y_pred)\n    return bce_loss + dice_loss\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:14.611303Z","iopub.execute_input":"2025-02-03T16:20:14.611991Z","iopub.status.idle":"2025-02-03T16:20:14.631475Z","shell.execute_reply.started":"2025-02-03T16:20:14.611956Z","shell.execute_reply":"2025-02-03T16:20:14.630612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv3DTranspose, concatenate, Dropout, Input, Add\nfrom tensorflow.keras.models import Model\n\ndef double_conv_block(x, filters, kernel_size=(3, 3, 3), padding='same', kernel_initializer='he_normal', dropout_rate=0.1):\n    # First convolution\n    x = Conv3D(filters, kernel_size, activation='relu', padding=padding, kernel_initializer=kernel_initializer)(x)\n    x = Dropout(dropout_rate)(x)\n    # Second convolution\n    x = Conv3D(filters, kernel_size, activation='relu', padding=padding, kernel_initializer=kernel_initializer)(x)\n    x = Dropout(dropout_rate)(x)\n    return x\n\ndef residual_block(x, filters, kernel_size=(3, 3, 3), padding='same', kernel_initializer='he_normal', dropout_rate=0.1):\n    # Double convolution\n    conv_out = double_conv_block(x, filters, kernel_size, padding, kernel_initializer, dropout_rate)\n    # Residual connection\n    residual = Conv3D(filters, (1, 1, 1), padding=padding, kernel_initializer=kernel_initializer)(x)\n    out = Add()([conv_out, residual])\n    return out\n\ndef build_unet(inputs, ker_init):\n    num_classes = 4\n    s = inputs  # (IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH, IMG_CHANNELS)\n\n    # Contraction path\n    c1 = residual_block(s, 16, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.1)\n    p1 = MaxPooling3D((2, 2, 2))(c1)\n    \n    c2 = residual_block(p1, 32, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.1)\n    p2 = MaxPooling3D((2, 2, 2))(c2)\n     \n    c3 = residual_block(p2, 64, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.2)\n    p3 = MaxPooling3D((2, 2, 2))(c3)\n     \n    c4 = residual_block(p3, 128, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.2)\n    p4 = MaxPooling3D(pool_size=(2, 2, 2))(c4)\n     \n    c5 = residual_block(p4, 256, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.3)\n    \n    # Expansive path \n    u6 = Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = residual_block(u6, 128, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.2)\n     \n    u7 = Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = residual_block(u7, 64, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.2)\n     \n    u8 = Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = residual_block(u8, 32, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.1)\n     \n    u9 = Conv3DTranspose(16, (2, 2, 2), strides=(2, 2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1])\n    c9 = residual_block(u9, 16, kernel_size=(3, 3, 3), kernel_initializer=ker_init, dropout_rate=0.1)\n     \n    outputs = Conv3D(num_classes, (1, 1, 1), activation='softmax')(c9)\n     \n    model = Model(inputs=[inputs], outputs=[outputs])\n    model.summary()\n    \n    return model\n\n# Define input layer\ninput_layer = Input((IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH, 3))\nmodel = build_unet(input_layer, 'he_normal')\n\n# Compile the model\nmodel.compile(\n    loss=bce_dice_loss,  # Use your custom loss function\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.MeanIoU(num_classes=4),\n        dice_coef,\n        precision,\n        sensitivity,\n        specificity,\n        dice_coef_per_class(0, \"dice_coef_necrotic\"),  # Necrotic Core\n        dice_coef_per_class(1, \"dice_coef_edema\"),     # Peritumoral Edema\n        dice_coef_per_class(2, \"dice_coef_enhancing\"), # Enhancing Tumor\n        jaccard_coef\n    ]\n)\n\nprint(model.input_shape)\nprint(model.output_shape)","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:14.632845Z","iopub.execute_input":"2025-02-03T16:20:14.63319Z","iopub.status.idle":"2025-02-03T16:20:14.958088Z","shell.execute_reply.started":"2025-02-03T16:20:14.633155Z","shell.execute_reply":"2025-02-03T16:20:14.957123Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lists of directories with studies\ntrain_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n#train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\ntrain_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); \n\n    \ntrain_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \ntrain_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) ","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:14.959142Z","iopub.execute_input":"2025-02-03T16:20:14.959395Z","iopub.status.idle":"2025-02-03T16:20:15.165165Z","shell.execute_reply.started":"2025-02-03T16:20:14.959372Z","shell.execute_reply":"2025-02-03T16:20:15.164398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_test_ids = ['BraTS20_Training_319', 'BraTS20_Training_252', 'BraTS20_Training_195', 'BraTS20_Training_163', 'BraTS20_Training_213', 'BraTS20_Training_209', 'BraTS20_Training_337', 'BraTS20_Training_058', 'BraTS20_Training_278', 'BraTS20_Training_025', 'BraTS20_Training_143', 'BraTS20_Training_193', 'BraTS20_Training_008', 'BraTS20_Training_024', 'BraTS20_Training_219', 'BraTS20_Training_266', 'BraTS20_Training_282', 'BraTS20_Training_097', 'BraTS20_Training_217', 'BraTS20_Training_153', 'BraTS20_Training_299', 'BraTS20_Training_035', 'BraTS20_Training_349', 'BraTS20_Training_129', 'BraTS20_Training_115', 'BraTS20_Training_002', 'BraTS20_Training_311', 'BraTS20_Training_265', 'BraTS20_Training_231', 'BraTS20_Training_020', 'BraTS20_Training_031', 'BraTS20_Training_036', 'BraTS20_Training_131', 'BraTS20_Training_204', 'BraTS20_Training_347', 'BraTS20_Training_155', 'BraTS20_Training_161', 'BraTS20_Training_041', 'BraTS20_Training_107', 'BraTS20_Training_296', 'BraTS20_Training_047', 'BraTS20_Training_238', 'BraTS20_Training_322', 'BraTS20_Training_016', 'BraTS20_Training_060']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T17:51:12.280727Z","iopub.execute_input":"2025-02-03T17:51:12.281074Z","iopub.status.idle":"2025-02-03T17:51:12.286904Z","shell.execute_reply.started":"2025-02-03T17:51:12.281047Z","shell.execute_reply":"2025-02-03T17:51:12.286005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Define the output directory path\noutput_dir = '/kaggle/working/'\n\n# Function to save IDs to a text file in the output directory\ndef save_ids_to_file(ids, file_name):\n    file_path = os.path.join(output_dir, file_name)\n    with open(file_path, 'w') as file:\n        for id_ in ids:\n            file.write(id_ + '\\n')\n\n# Print the number of images in each set\nprint(\"Number of images in training set:\", len(train_ids))\nprint(\"Number of images in validation set:\", len(val_ids))\nprint(\"Number of images in test set:\", len(test_ids))\n","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:15.166451Z","iopub.execute_input":"2025-02-03T16:20:15.166832Z","iopub.status.idle":"2025-02-03T16:20:15.174113Z","shell.execute_reply.started":"2025-02-03T16:20:15.166797Z","shell.execute_reply":"2025-02-03T16:20:15.173159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ResizeAndReduceSlices(nib,size,slices,gap):\n    imgs = []\n    for i in range(128):\n        list1 = []\n        for j in range(128):\n            list2 = []\n            for k in range(48):\n                list2.append(0)\n            list1.append(list2)\n        imgs.append(list1)\n    imgs = np.array(imgs)\n    for i in range(slices):\n        imgs[:,:,i] = cv2.resize(nib[:,:,int((i*gap)+20)],(size,size))\n    return imgs","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:15.17527Z","iopub.execute_input":"2025-02-03T16:20:15.175559Z","iopub.status.idle":"2025-02-03T16:20:15.18511Z","shell.execute_reply.started":"2025-02-03T16:20:15.175536Z","shell.execute_reply":"2025-02-03T16:20:15.184233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, dim=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH), batch_size = 1, n_channels = 3, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        Batch_ids = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(Batch_ids)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, Batch_ids):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.zeros((self.batch_size, *self.dim, self.n_channels)) # shape of (5, 128, 128, 128, 3)\n        #y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n        Y = np.zeros((self.batch_size, *self.dim)) # shape of (5, 128, 128, 128)\n        \n        # Generate data\n        for c, i in enumerate(Batch_ids):\n            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n\n            data_path = os.path.join(case_path, f'{i}_flair.nii');\n            flair = nib.load(data_path).get_fdata() \n\n            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n            ce = nib.load(data_path).get_fdata()\n\n            data_path = os.path.join(case_path, f'{i}_t2.nii'); \n            t2 = nib.load(data_path).get_fdata()\n            \n            data_path = os.path.join(case_path, f'{i}_seg.nii');\n            seg = nib.load(data_path).get_fdata()\n\n            #seg=seg.astype(np.uint8)\n            #seg[seg==4] = 3\n\n            #temp_combined_images = np.stack([flair, ce, t2], axis=3)\n            #temp_combined_images=temp_combined_images[56:184, 56:184, 13:141]\n            #temp_mask = seg[56:184, 56:184, 13:141]\n            \n            slice_w = 25\n\n            # (j +VOLUME_SLICES*c) \n            for j in range(VOLUME_SLICES):\n                X[c,:,:,j,0] = cv2.resize(flair[:,:,VOLUME_START_AT+int(j*2.5)], (IMG_HEIGHT, IMG_WIDTH));\n                X[c,:,:,j,1] = cv2.resize(ce[:,:,VOLUME_START_AT+int(j*2.5)], (IMG_HEIGHT, IMG_WIDTH));\n                X[c,:,:,j,2] = cv2.resize(t2[:,:,VOLUME_START_AT+int(j*2.5)], (IMG_HEIGHT, IMG_WIDTH));\n\n                #y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n                Y[c,:,:,j] = cv2.resize(seg[:,:,VOLUME_START_AT+int(j*2.5)], (IMG_HEIGHT, IMG_WIDTH));\n                    \n        # Generate masks\n        #y[y==4] = 3;\n        Y[Y==4] = 3;\n        mask = tf.one_hot(Y, 4);\n        #Y = tf.image.resize(mask, (IMG_HEIGHT, IMG_WIDTH));\n        return X/np.max(X), mask\n        #return temp_combined_images/np.max(temp_combined_images), temp_mask\n        \ntraining_generator = DataGenerator(train_ids)\nvalid_generator = DataGenerator(val_ids)\ntest_generator = DataGenerator(test_ids)\nmy_test_generator=DataGenerator(my_test_ids)","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:51:16.01681Z","iopub.execute_input":"2025-02-03T17:51:16.017153Z","iopub.status.idle":"2025-02-03T17:51:16.032088Z","shell.execute_reply.started":"2025-02-03T17:51:16.017124Z","shell.execute_reply":"2025-02-03T17:51:16.031214Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_ids))\nprint(len(val_ids))\nprint(len(test_ids))\nprint(len(training_generator))\nprint(len(valid_generator))\nprint(len(test_generator))\nimg, mask = test_generator[0]\nmask=np.argmax(mask, axis=4)[0,:,:,:]\nprint(mask.shape)\nfor n_slice in range(48):\n# n_slice = 24\n    plt.figure(figsize=(12, 8))\n    plt.subplot(231)\n    plt.title('Testing Image')\n    plt.imshow(img[0,:,:,n_slice,0], cmap='gray')\n    plt.subplot(232)\n    plt.title('Testing Label')\n    plt.imshow(mask[:,:,n_slice])","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:15.206589Z","iopub.execute_input":"2025-02-03T16:20:15.206887Z","iopub.status.idle":"2025-02-03T16:20:25.226816Z","shell.execute_reply.started":"2025-02-03T16:20:15.206863Z","shell.execute_reply":"2025-02-03T16:20:25.225885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show number of data for each dir \ndef showDataLayout():\n    plt.bar([\"Train\",\"Valid\",\"Test\"],\n    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n    plt.legend()\n\n    plt.ylabel('Number of images')\n    plt.title('Data distribution')\n    plt.savefig('data2020.png')\n    plt.show()\n    \nshowDataLayout()","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:25.227956Z","iopub.execute_input":"2025-02-03T16:20:25.228202Z","iopub.status.idle":"2025-02-03T16:20:25.426007Z","shell.execute_reply.started":"2025-02-03T16:20:25.22818Z","shell.execute_reply":"2025-02-03T16:20:25.425069Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history =  model.fit(training_generator,\n                    epochs=50,\n                    steps_per_epoch=len(train_ids),\n                    validation_data = valid_generator\n                    )","metadata":{"execution":{"iopub.status.busy":"2025-02-03T16:20:25.427079Z","iopub.execute_input":"2025-02-03T16:20:25.427314Z","iopub.status.idle":"2025-02-03T17:33:34.633316Z","shell.execute_reply.started":"2025-02-03T16:20:25.427292Z","shell.execute_reply":"2025-02-03T17:33:34.632514Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"model_my_3D_UNet.h5\")","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:33:34.634794Z","iopub.execute_input":"2025-02-03T17:33:34.635063Z","iopub.status.idle":"2025-02-03T17:33:34.935661Z","shell.execute_reply.started":"2025-02-03T17:33:34.635038Z","shell.execute_reply":"2025-02-03T17:33:34.934838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"############ load trained model ################\nmodel = tf.keras.models.load_model(\n    '/kaggle/working/model_my_3D_UNet.h5',\n    custom_objects={\n        'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=4),  # Use 'MeanIoU' instead of 'accuracy'\n        'dice_coef': dice_coef,\n        'precision': precision,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'dice_coef_necrotic': dice_coef_per_class(0, \"dice_coef_necrotic\"),  # Necrotic Core (class 0)\n        'dice_coef_edema': dice_coef_per_class(1, \"dice_coef_edema\"),         # Peritumoral Edema (class 1)\n        'dice_coef_enhancing': dice_coef_per_class(2, \"dice_coef_enhancing\")  # Enhancing Tumor (class 2)\n    },\n    compile=False  # Do not compile the model during loading\n)\n\n\n\nhist=history.history\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,4,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\nax[3].plot(epoch,hist['mean_io_u_8'],'b',label='Training mean IOU')\nax[3].plot(epoch,hist['val_mean_io_u_8'],'r',label='Validation mean IOU')\nax[3].legend()\nplt.savefig('training_result_2018.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:35:52.122655Z","iopub.execute_input":"2025-02-03T17:35:52.123205Z","iopub.status.idle":"2025-02-03T17:35:53.157695Z","shell.execute_reply.started":"2025-02-03T17:35:52.123153Z","shell.execute_reply":"2025-02-03T17:35:53.15679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(hist)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:33:35.733841Z","iopub.status.idle":"2025-02-03T17:33:35.734309Z","shell.execute_reply.started":"2025-02-03T17:33:35.734046Z","shell.execute_reply":"2025-02-03T17:33:35.734066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.models.load_model(\n    '/kaggle/working/model_my_3D_UNet.h5',\n    custom_objects={\n        'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=4),  # Use 'MeanIoU' instead of 'accuracy'\n        'dice_coef': dice_coef,\n        'precision': precision,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'dice_coef_necrotic': dice_coef_per_class(0, \"dice_coef_necrotic\"),  # Necrotic Core (class 0)\n        'dice_coef_edema': dice_coef_per_class(1, \"dice_coef_edema\"),         # Peritumoral Edema (class 1)\n        'dice_coef_enhancing': dice_coef_per_class(2, \"dice_coef_enhancing\")  # Enhancing Tumor (class 2)\n    },\n    compile=False  # Do not compile the model during loading\n)\nmodel.compile(\n    loss=bce_dice_loss,  # Use your custom loss function\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.MeanIoU(num_classes=4),\n        dice_coef,\n        precision,\n        sensitivity,\n        specificity,\n        dice_coef_per_class(0, \"dice_coef_necrotic\"),  # Necrotic Core\n        dice_coef_per_class(1, \"dice_coef_edema\"),     # Peritumoral Edema\n        dice_coef_per_class(2, \"dice_coef_enhancing\"), # Enhancing Tumor\n        jaccard_coef\n    ]\n)\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(test_generator, batch_size=100)\nprint(\"test loss, test acc:\", results)","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:51:32.929191Z","iopub.execute_input":"2025-02-03T17:51:32.92958Z","iopub.status.idle":"2025-02-03T17:51:43.135155Z","shell.execute_reply.started":"2025-02-03T17:51:32.929548Z","shell.execute_reply":"2025-02-03T17:51:43.134219Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataGenerator(ex: test_generator)=>tuple(images,mask)\n# images=>[slice,depth,width,height,channels]\n# mask=>[slice,depth,width,height,labels]\n\nimg_num = 33\ntest_img = test_generator.__getitem__(img_num)[0]    #tuple(image,mask) -> 0: image\ntest_mask = test_generator.__getitem__(img_num)[1]   #tuple(image,mask) -> 1: mask\ntest_mask_argmax=np.argmax(test_mask, axis=4)[0,:,:,:]\n\ntest_prediction = model.predict(test_img)\ntest_prediction_argmax=np.argmax(test_prediction, axis=4)[0,:,:,:]\n\nprint(\"test_prediction_shape: \" + str(test_prediction.shape))\nprint(\"test_prediction_argmax_shape: \" + str(test_prediction_argmax.shape))\nprint(np.unique(test_prediction_argmax))\nprint(\"test_img_shape: \" + str(test_img.shape))\nprint(\"test_mask_shape: \" + str(test_mask.shape))\nprint(\"test_mask_argmax: \" + str(test_mask_argmax.shape))\n\nn_slice = 15\nfor n_slice in range(48):\n    plt.figure(figsize=(12, 8))\n    plt.subplot(231)\n    plt.title('Testing Image')\n    plt.imshow(test_img[0,:,:,n_slice,0], cmap='gray')\n    plt.subplot(232)\n    plt.title('Testing Label')\n    plt.imshow(test_mask_argmax[:,:,n_slice])\n    plt.subplot(233)\n    plt.title('Prediction on test image')\n    plt.imshow(test_prediction_argmax[:,:,n_slice])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:51:46.312695Z","iopub.execute_input":"2025-02-03T17:51:46.313315Z","iopub.status.idle":"2025-02-03T17:52:02.171957Z","shell.execute_reply.started":"2025-02-03T17:51:46.313281Z","shell.execute_reply":"2025-02-03T17:52:02.171111Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_num = 15\ntest_img = test_generator.__getitem__(img_num)[0]    #tuple(image,mask) -> 0: image\ntest_mask = test_generator.__getitem__(img_num)[1]   #tuple(image,mask) -> 1: mask\ntest_mask_argmax=np.argmax(test_mask, axis=4)[0,:,:,:]\n\ntest_prediction = model.predict(test_img)\ntest_prediction_argmax=np.argmax(test_prediction, axis=4)[0,:,:,:]\n\nprint(\"test_prediction_shape: \" + str(test_prediction.shape))\nprint(\"test_prediction_argmax_shape: \" + str(test_prediction_argmax.shape))\nprint(np.unique(test_prediction_argmax))\nprint(\"test_img_shape: \" + str(test_img.shape))\nprint(\"test_mask_shape: \" + str(test_mask.shape))\nprint(\"test_mask_argmax: \" + str(test_mask_argmax.shape))\n\nslice_index = 15  # Specify the index of the slice you want to display\n\nplt.figure(figsize=(12, 8))\nplt.subplot(231)\nplt.title('Testing Image')\nplt.imshow(test_img[0,:,:,slice_index,0], cmap='gray')\nplt.subplot(232)\nplt.title('Testing Label')\nplt.imshow(test_mask_argmax[:,:,slice_index])\nplt.subplot(233)\nplt.title('Prediction on test image')\nplt.imshow(test_prediction_argmax[:,:,slice_index])\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-03T17:52:05.269314Z","iopub.execute_input":"2025-02-03T17:52:05.269706Z","iopub.status.idle":"2025-02-03T17:52:06.070524Z","shell.execute_reply.started":"2025-02-03T17:52:05.269678Z","shell.execute_reply":"2025-02-03T17:52:06.069581Z"},"trusted":true},"outputs":[],"execution_count":null}]}